# Parallel-and-Distributed-Algorithms

## Official Course Description

Computers are becoming increasingly parallel, with many cores or processors working concurrently to perform a single task. In order to utilize the full power of modern computers, it is essential to write programs that exploit parallelism. Yet utilizing the power of parallelism requires different ways of reasoning about problems, and parallelism introduces subtleties not present in sequential (i.e., non-parallel) programming. This course will introduce you to the art and science of writing parallel programs. We will consider both practical and theoretical aspects of parallel computing, and create software that is many times faster than any sequential program performing the same task.

### The core topics covered in the course include:

Multithreaded programming in Java,
mutual exclusion,
concurrent objects,
locks and contention resolution,
blocking synchronization,
concurrent data structures (linked lists, queues, stacks, hash tables, and sets, skiplists and balanced search)
scheduling, work distribution, and barriers,
data parallelism (MapReduce and streams),
SIMD parallelism/vector operations.

Link: https://willrosenbaum.com/teaching/2023s-cosc-273/syllabus/
